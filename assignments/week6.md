Weekly Homework #6
==
Due Friday, October 6, at 11:55pm.

A. If the statement below is false, explain why it is false.  Otherwise, briefly explain why it is true.

1.  One-vs-all classification decides which label to predict by using the most confident classifiers to vote on the correct answer.

2.  Error correcting code classification constructs multiple binary classifiers for choosing a label.

3. 1-vs-1 classification selects the classifier with the highest confidence in the correct answer.

4. Standard support vector machines produce probabilities, just like logistic regression and the perceptron.

5. Logistic regression can be viewed as a special case of the perception.

B. Read the following paper:

https://aclweb.org/anthology/K/K16/K16-1010.pdf

1. What is the objective of this paper?

2. What kind of model does the paper use?

3. What is a "baseline," and how is it used in this paper?

4. What multi-class classification scheme does the paper use for 50 verbs?

5. What multi-class classification scheme does the paper use for the multiple choice scenario?  Describe how this works.

6. How does the computational performance on the multiple choice data compare with the huamn performance?

7. What is the feature set, and which features seem to be particularly helpful?

8. What is the difference between the full context set and the random length set?

9. Are the changes in confidences in the answer that are mentioned in the analysis section directly comparable when using one-vs-all?  Briefly argue your case.

10. In equation 1, what are the prior and the posterior, respectively, and what do they represent? 

11. Why does the paper suggest that the baseline model doesn't work well?

12. What does the paper mean when in Figure 3, when it describes the distribution as "Zipfian?"
  
